{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.context import SparkContext"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3afd4d80-d2fb-4db1-b39f-abc86fe7232e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["spark = SparkSession.builder.appName('cas').getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5efe7f30-02e7-4fdd-9ca1-f02a23ae23a5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c20ebb6-b8ab-468e-80ff-1f2d679b2bd9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_train = spark.read.csv(\"/FileStore/tables/train.csv\", header=True, inferSchema=True)\ndf_train.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a65b7886-1df6-44c5-bd2e-8670bc5f11c2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+---+------------+------------+-------+------+---+---------------+---------+-------------------------+-------------------+--------------------+-----------------+-------------+------------------+---------+------+------------------------+-----+----------+-----+--------+---------+---------------+----+-----+----------------+-----+----------+-------------+---------------------+-----------+----------------+--------+-------------+-----------+----------+--------------------+-------------+--------+-----------+---------+---------+------------------+-----------+----------+-----------+-----------+---+----+--------------------+---------------+-------------+-----------+-----+----+-----+------------------+-----+------------+-------+----------+--------+-----------+\n|_c0|           X|           Y|bicycle|bridge|bus|carStationWagon|cliffBank|crashDirectionDescription| crashFinancialYear|      crashLocation1|   crashLocation2|crashSeverity|crashSHDescription|crashYear|debris|directionRoleDescription|ditch|fatalCount|fence|flatHill|guardRail|houseOrBuilding|kerb|light|minorInjuryCount|moped|motorcycle|NumberOfLanes|objectThrownOrDropped|otherObject|otherVehicleType|overBank|parkedVehicle|phoneBoxEtc|postOrPole|              region|roadCharacter|roadLane|roadSurface|roadworks|schoolBus|seriousInjuryCount|slipOrFlood|speedLimit|strayAnimal|streetLight|suv|taxi|             tlaName| trafficControl|trafficIsland|trafficSign|train|tree|truck|unknownVehicleType|urban|vanOrUtility|vehicle|waterRiver|weatherA|claimAmount|\n+---+------------+------------+-------+------+---+---------------+---------+-------------------------+-------------------+--------------------+-----------------+-------------+------------------+---------+------+------------------------+-----+----------+-----+--------+---------+---------------+----+-----+----------------+-----+----------+-------------+---------------------+-----------+----------------+--------+-------------+-----------+----------+--------------------+-------------+--------+-----------+---------+---------+------------------+-----------+----------+-----------+-----------+---+----+--------------------+---------------+-------------+-----------+-----+----+-----+------------------+-----+------------+-------+----------+--------+-----------+\n|  5|   1838932.0|   5519398.0|    0.0|   0.0|0.0|            1.0|      0.0|                     East|  4.124430773613714|PAHIATUA MANGAHAO...| RIDGE ROAD NORTH|            1|                 0|     2014|   0.0|                   North|  0.0|       0.0|  1.0|       0|      0.0|            0.0| 0.0|    2|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     1.0|          0.0|        0.0|       0.0|Manawatū-Whanganu...|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|    Tararua District|        Unknown|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       2|  2445277.0|\n| 13|   1574177.0|   5179893.0|    0.0|   0.0|0.0|            1.0|      0.0|                     East| -9.875569226386286|         RASEN PLACE|        JOLLIE ST|            3|                 0|     2000|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 1.0|    1|             0.0|  0.0|       0.0|          1.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Canterbury Region|          Nil|       1|     Sealed|      0.0|      0.0|               1.0|        0.0|      50.0|        0.0|         On|0.0| 0.0|   Christchurch City|        Unknown|          0.0|        0.0|  0.0| 1.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  9666791.0|\n| 17|   1767091.0|   5912669.0|    0.0|   0.0|0.0|            1.0|      0.0|                    South| 0.1244307736137138|        CARDIFF ROAD|      CINDY PLACE|            1|                 0|     2010|   0.0|                   South|  0.0|       0.0|  1.0|       1|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       1.0|     Auckland Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|        Off|0.0| 0.0|            Auckland|       Give way|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  3703142.0|\n| 18|   1746990.0|   5915134.0|    0.0|   0.0|0.0|            2.0|      0.0|                     None| -4.875569226386286|    GREAT NORTH ROAD|     HEPBURN ROAD|            1|                 0|     2006|   0.0|                    East|  0.0|       0.0|  0.0|       1|      0.0|            0.0| 0.0|    0|             2.0|  0.0|       0.0|          4.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Auckland Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|        Off|0.0| 0.0|            Auckland|Traffic Signals|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  8725516.0|\n| 20|   1746822.0|   5918596.0|    0.0|   0.0|0.0|            2.0|      0.0|                    South| -4.875569226386286|       TE ATATU ROAD|  WAKELING AVENUE|            1|                 0|     2006|   0.0|                   South|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             3.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Auckland Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|            Auckland|        Unknown|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|1.3734963E7|\n| 26|   1767320.0|   5901246.0|    0.0|   0.0|0.0|            1.0|      0.0|                     None|  8.124430773613714|        RUSSELL ROAD|        KENT ROAD|            3|                 0|     2018|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             5.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Auckland Region|          Nil|       1|     Sealed|      0.0|      0.0|               1.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|            Auckland|            Nil|          0.0|        0.0|  0.0| 0.0|  1.0|               0.0|Urban|         0.0|    0.0|       0.0|       3|2.0053408E7|\n| 33|   1821245.0|   5461836.0|    0.0|   0.0|0.0|            1.0|      0.0|                     None| 0.1244307736137138|          SOUTH BELT|    MANCHESTER ST|            1|                 0|     2010|   0.0|                   South|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Wellington Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|  Masterton District|            Nil|          0.0|        0.0|  0.0| 0.0|  1.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  3595965.0|\n| 38|   1755196.0|   5434483.0|    0.0|   0.0|0.0|            2.0|      0.0|                     None| -8.875569226386286|                SH 2|    HOROKIWI ROAD|            1|                 2|     2002|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             3.0|  0.0|       0.0|          4.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Wellington Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|     Lower Hutt City|       Give way|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|  7418493.0|\n| 48|   1759917.0|   5436791.0|    0.0|   0.0|0.0|            1.0|      0.0|                     West|-10.875569226386286|        MELLING LINK|    RUTHERFORD ST|            1|                 0|     2000|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Wellington Region|       Bridge|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|     Lower Hutt City|            Nil|          0.0|        0.0|  0.0| 0.0|  1.0|               0.0|Urban|         1.0|    0.0|       0.0|       0|  4730658.0|\n| 50|   1761478.0|   5914197.0|    0.0|   0.0|0.0|            1.0|      0.0|                    South| -2.875569226386286|               SH 1N|      PENROSE INT|            1|                 2|     2008|   0.0|                   North|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             1.0|  0.0|       0.0|          3.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Auckland Region|          Nil|       0|     Sealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|        Off|0.0| 0.0|            Auckland|            Nil|          0.0|        0.0|  0.0| 0.0|  1.0|               0.0| Open|         0.0|    0.0|       0.0|       2|  2296355.0|\n| 52|   1723203.0|   6003542.0|    0.0|   0.0|0.0|            0.0|      0.0|                     East|  6.124430773613714|               SH 12| DAVIS LANDING ST|            1|                 2|     2017|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|    Northland Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      70.0|        0.0|       None|0.0| 0.0|    Kaipara District|        Unknown|          0.0|        0.0|  0.0| 0.0|  1.0|               0.0|Urban|         1.0|    0.0|       0.0|       0|  2818103.0|\n| 65|   1653431.0|   5310577.0|    0.0|   0.0|0.0|            0.0|      0.0|                     None| 0.1244307736137138|    MOUNT FYFFE ROAD|    POSTMANS ROAD|            1|                 0|     2011|   0.0|                   South|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             3.0|  0.0|       1.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Canterbury Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|   Kaikoura District|       Give way|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|  6663858.0|\n| 67|   1823725.0|   5450095.0|    0.0|   0.0|0.0|            0.0|      0.0|                    North| 0.1244307736137138|       TE WHITI ROAD|     WAIPOPO ROAD|            3|                 0|     2011|   0.0|                   South|  0.0|       0.0|  0.0|       1|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       1.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Wellington Region|          Nil|       1|     Sealed|      0.0|      0.0|               1.0|        0.0|     100.0|        0.0|       None|1.0| 0.0|  Carterton District|            Nil|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|1.0775363E7|\n| 69|   1692048.0|   5675715.0|    0.0|   0.0|0.0|            1.0|      0.0|                     None|  6.124430773613714|               SH 45|        VIVIAN ST|            1|                 2|     2016|   0.0|                    West|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       0.0|          4.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Taranaki Region|          Nil|       0|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|New Plymouth Dist...|Traffic Signals|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0|Urban|         1.0|    0.0|       0.0|       0|  4250464.0|\n| 72|   1237393.0|   4851266.0|    0.0|   0.0|0.0|            1.0|      0.0|                     East|  7.124430773613714|          FERRY ROAD|    STAUNTON ROAD|            3|                 0|     2017|   0.0|                    West|  1.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             0.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|    Southland Region|          Nil|       1|     Sealed|      0.0|      0.0|               1.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|   Invercargill City|        Unknown|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|  8223092.0|\n| 76|   1751529.0|   5917960.0|    0.0|   0.0|0.0|            2.0|      0.0|                     East|-10.875569226386286|               SH 16| WATERVIEW ON WBD|            1|                 2|     2000|   0.0|                    West|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             2.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Auckland Region|          Nil|       0|     Sealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|        Off|0.0| 0.0|            Auckland|            Nil|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|  7561138.0|\n| 78|   1801669.0|   5817640.0|    0.0|   0.0|0.0|            1.0|      0.0|                     East| -7.875569226386286|        HOLLAND ROAD|       HUME PLACE|            1|                 0|     2003|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    1|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|      Waikato Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|       Hamilton City|            Nil|          0.0|        0.0|  0.0| 1.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  3491661.0|\n| 80|1600786.9627|5459197.3309|    1.0|   0.0|1.0|            0.0|      0.0|                     East| 10.124430773613714|     TOKONGAWA DRIVE|LADY BARKLY GROVE|            1|                 0|     2021|   0.0|                   South|  0.0|       0.0|  0.0|       1|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|       Tasman Region|          Nil|       1|     Sealed|      0.0|      1.0|               0.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|     Tasman District|            Nil|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|  3254782.0|\n| 89|   1603983.0|   6144272.0|    0.0|   0.0|0.0|            0.0|      0.0|                    North| -7.875569226386286|       Z BCH 90 MILE|    HUKATERE ROAD|            1|                 0|     2002|   0.0|                   South|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             3.0|  0.0|       0.0|          0.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|    Northland Region|          Nil|       3|   Unsealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|  Far North District|            Nil|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         1.0|    0.0|       0.0|       0|1.1874975E7|\n| 91|   1804647.0|   5790812.0|    0.0|   0.0|0.0|            2.0|      0.0|                     None| -2.875569226386286|      CAMBRIDGE ROAD|     ROGERS PLACE|            1|                 0|     2008|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|      Waikato Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|      Waipa District|            Nil|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  4596395.0|\n+---+------------+------------+-------+------+---+---------------+---------+-------------------------+-------------------+--------------------+-----------------+-------------+------------------+---------+------+------------------------+-----+----------+-----+--------+---------+---------------+----+-----+----------------+-----+----------+-------------+---------------------+-----------+----------------+--------+-------------+-----------+----------+--------------------+-------------+--------+-----------+---------+---------+------------------+-----------+----------+-----------+-----------+---+----+--------------------+---------------+-------------+-----------+-----+----+-----+------------------+-----+------------+-------+----------+--------+-----------+\nonly showing top 20 rows\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+---+------------+------------+-------+------+---+---------------+---------+-------------------------+-------------------+--------------------+-----------------+-------------+------------------+---------+------+------------------------+-----+----------+-----+--------+---------+---------------+----+-----+----------------+-----+----------+-------------+---------------------+-----------+----------------+--------+-------------+-----------+----------+--------------------+-------------+--------+-----------+---------+---------+------------------+-----------+----------+-----------+-----------+---+----+--------------------+---------------+-------------+-----------+-----+----+-----+------------------+-----+------------+-------+----------+--------+-----------+\n|_c0|           X|           Y|bicycle|bridge|bus|carStationWagon|cliffBank|crashDirectionDescription| crashFinancialYear|      crashLocation1|   crashLocation2|crashSeverity|crashSHDescription|crashYear|debris|directionRoleDescription|ditch|fatalCount|fence|flatHill|guardRail|houseOrBuilding|kerb|light|minorInjuryCount|moped|motorcycle|NumberOfLanes|objectThrownOrDropped|otherObject|otherVehicleType|overBank|parkedVehicle|phoneBoxEtc|postOrPole|              region|roadCharacter|roadLane|roadSurface|roadworks|schoolBus|seriousInjuryCount|slipOrFlood|speedLimit|strayAnimal|streetLight|suv|taxi|             tlaName| trafficControl|trafficIsland|trafficSign|train|tree|truck|unknownVehicleType|urban|vanOrUtility|vehicle|waterRiver|weatherA|claimAmount|\n+---+------------+------------+-------+------+---+---------------+---------+-------------------------+-------------------+--------------------+-----------------+-------------+------------------+---------+------+------------------------+-----+----------+-----+--------+---------+---------------+----+-----+----------------+-----+----------+-------------+---------------------+-----------+----------------+--------+-------------+-----------+----------+--------------------+-------------+--------+-----------+---------+---------+------------------+-----------+----------+-----------+-----------+---+----+--------------------+---------------+-------------+-----------+-----+----+-----+------------------+-----+------------+-------+----------+--------+-----------+\n|  5|   1838932.0|   5519398.0|    0.0|   0.0|0.0|            1.0|      0.0|                     East|  4.124430773613714|PAHIATUA MANGAHAO...| RIDGE ROAD NORTH|            1|                 0|     2014|   0.0|                   North|  0.0|       0.0|  1.0|       0|      0.0|            0.0| 0.0|    2|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     1.0|          0.0|        0.0|       0.0|Manawatū-Whanganu...|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|    Tararua District|        Unknown|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       2|  2445277.0|\n| 13|   1574177.0|   5179893.0|    0.0|   0.0|0.0|            1.0|      0.0|                     East| -9.875569226386286|         RASEN PLACE|        JOLLIE ST|            3|                 0|     2000|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 1.0|    1|             0.0|  0.0|       0.0|          1.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Canterbury Region|          Nil|       1|     Sealed|      0.0|      0.0|               1.0|        0.0|      50.0|        0.0|         On|0.0| 0.0|   Christchurch City|        Unknown|          0.0|        0.0|  0.0| 1.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  9666791.0|\n| 17|   1767091.0|   5912669.0|    0.0|   0.0|0.0|            1.0|      0.0|                    South| 0.1244307736137138|        CARDIFF ROAD|      CINDY PLACE|            1|                 0|     2010|   0.0|                   South|  0.0|       0.0|  1.0|       1|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       1.0|     Auckland Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|        Off|0.0| 0.0|            Auckland|       Give way|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  3703142.0|\n| 18|   1746990.0|   5915134.0|    0.0|   0.0|0.0|            2.0|      0.0|                     None| -4.875569226386286|    GREAT NORTH ROAD|     HEPBURN ROAD|            1|                 0|     2006|   0.0|                    East|  0.0|       0.0|  0.0|       1|      0.0|            0.0| 0.0|    0|             2.0|  0.0|       0.0|          4.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Auckland Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|        Off|0.0| 0.0|            Auckland|Traffic Signals|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  8725516.0|\n| 20|   1746822.0|   5918596.0|    0.0|   0.0|0.0|            2.0|      0.0|                    South| -4.875569226386286|       TE ATATU ROAD|  WAKELING AVENUE|            1|                 0|     2006|   0.0|                   South|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             3.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Auckland Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|            Auckland|        Unknown|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|1.3734963E7|\n| 26|   1767320.0|   5901246.0|    0.0|   0.0|0.0|            1.0|      0.0|                     None|  8.124430773613714|        RUSSELL ROAD|        KENT ROAD|            3|                 0|     2018|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             5.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Auckland Region|          Nil|       1|     Sealed|      0.0|      0.0|               1.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|            Auckland|            Nil|          0.0|        0.0|  0.0| 0.0|  1.0|               0.0|Urban|         0.0|    0.0|       0.0|       3|2.0053408E7|\n| 33|   1821245.0|   5461836.0|    0.0|   0.0|0.0|            1.0|      0.0|                     None| 0.1244307736137138|          SOUTH BELT|    MANCHESTER ST|            1|                 0|     2010|   0.0|                   South|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Wellington Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|  Masterton District|            Nil|          0.0|        0.0|  0.0| 0.0|  1.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  3595965.0|\n| 38|   1755196.0|   5434483.0|    0.0|   0.0|0.0|            2.0|      0.0|                     None| -8.875569226386286|                SH 2|    HOROKIWI ROAD|            1|                 2|     2002|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             3.0|  0.0|       0.0|          4.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Wellington Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|     Lower Hutt City|       Give way|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|  7418493.0|\n| 48|   1759917.0|   5436791.0|    0.0|   0.0|0.0|            1.0|      0.0|                     West|-10.875569226386286|        MELLING LINK|    RUTHERFORD ST|            1|                 0|     2000|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Wellington Region|       Bridge|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|     Lower Hutt City|            Nil|          0.0|        0.0|  0.0| 0.0|  1.0|               0.0|Urban|         1.0|    0.0|       0.0|       0|  4730658.0|\n| 50|   1761478.0|   5914197.0|    0.0|   0.0|0.0|            1.0|      0.0|                    South| -2.875569226386286|               SH 1N|      PENROSE INT|            1|                 2|     2008|   0.0|                   North|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             1.0|  0.0|       0.0|          3.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Auckland Region|          Nil|       0|     Sealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|        Off|0.0| 0.0|            Auckland|            Nil|          0.0|        0.0|  0.0| 0.0|  1.0|               0.0| Open|         0.0|    0.0|       0.0|       2|  2296355.0|\n| 52|   1723203.0|   6003542.0|    0.0|   0.0|0.0|            0.0|      0.0|                     East|  6.124430773613714|               SH 12| DAVIS LANDING ST|            1|                 2|     2017|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|    Northland Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      70.0|        0.0|       None|0.0| 0.0|    Kaipara District|        Unknown|          0.0|        0.0|  0.0| 0.0|  1.0|               0.0|Urban|         1.0|    0.0|       0.0|       0|  2818103.0|\n| 65|   1653431.0|   5310577.0|    0.0|   0.0|0.0|            0.0|      0.0|                     None| 0.1244307736137138|    MOUNT FYFFE ROAD|    POSTMANS ROAD|            1|                 0|     2011|   0.0|                   South|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             3.0|  0.0|       1.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Canterbury Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|   Kaikoura District|       Give way|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|  6663858.0|\n| 67|   1823725.0|   5450095.0|    0.0|   0.0|0.0|            0.0|      0.0|                    North| 0.1244307736137138|       TE WHITI ROAD|     WAIPOPO ROAD|            3|                 0|     2011|   0.0|                   South|  0.0|       0.0|  0.0|       1|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       1.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|   Wellington Region|          Nil|       1|     Sealed|      0.0|      0.0|               1.0|        0.0|     100.0|        0.0|       None|1.0| 0.0|  Carterton District|            Nil|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|1.0775363E7|\n| 69|   1692048.0|   5675715.0|    0.0|   0.0|0.0|            1.0|      0.0|                     None|  6.124430773613714|               SH 45|        VIVIAN ST|            1|                 2|     2016|   0.0|                    West|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       0.0|          4.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Taranaki Region|          Nil|       0|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|New Plymouth Dist...|Traffic Signals|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0|Urban|         1.0|    0.0|       0.0|       0|  4250464.0|\n| 72|   1237393.0|   4851266.0|    0.0|   0.0|0.0|            1.0|      0.0|                     East|  7.124430773613714|          FERRY ROAD|    STAUNTON ROAD|            3|                 0|     2017|   0.0|                    West|  1.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    2|             0.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|    Southland Region|          Nil|       1|     Sealed|      0.0|      0.0|               1.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|   Invercargill City|        Unknown|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|  8223092.0|\n| 76|   1751529.0|   5917960.0|    0.0|   0.0|0.0|            2.0|      0.0|                     East|-10.875569226386286|               SH 16| WATERVIEW ON WBD|            1|                 2|     2000|   0.0|                    West|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             2.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|     Auckland Region|          Nil|       0|     Sealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|        Off|0.0| 0.0|            Auckland|            Nil|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|  7561138.0|\n| 78|   1801669.0|   5817640.0|    0.0|   0.0|0.0|            1.0|      0.0|                     East| -7.875569226386286|        HOLLAND ROAD|       HUME PLACE|            1|                 0|     2003|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    1|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|      Waikato Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|       Hamilton City|            Nil|          0.0|        0.0|  0.0| 1.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  3491661.0|\n| 80|1600786.9627|5459197.3309|    1.0|   0.0|1.0|            0.0|      0.0|                     East| 10.124430773613714|     TOKONGAWA DRIVE|LADY BARKLY GROVE|            1|                 0|     2021|   0.0|                   South|  0.0|       0.0|  0.0|       1|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|       Tasman Region|          Nil|       1|     Sealed|      0.0|      1.0|               0.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|     Tasman District|            Nil|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         0.0|    0.0|       0.0|       0|  3254782.0|\n| 89|   1603983.0|   6144272.0|    0.0|   0.0|0.0|            0.0|      0.0|                    North| -7.875569226386286|       Z BCH 90 MILE|    HUKATERE ROAD|            1|                 0|     2002|   0.0|                   South|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             3.0|  0.0|       0.0|          0.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|    Northland Region|          Nil|       3|   Unsealed|      0.0|      0.0|               0.0|        0.0|     100.0|        0.0|       None|0.0| 0.0|  Far North District|            Nil|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0| Open|         1.0|    0.0|       0.0|       0|1.1874975E7|\n| 91|   1804647.0|   5790812.0|    0.0|   0.0|0.0|            2.0|      0.0|                     None| -2.875569226386286|      CAMBRIDGE ROAD|     ROGERS PLACE|            1|                 0|     2008|   0.0|                    East|  0.0|       0.0|  0.0|       0|      0.0|            0.0| 0.0|    0|             1.0|  0.0|       0.0|          2.0|                  0.0|        0.0|             0.0|     0.0|          0.0|        0.0|       0.0|      Waikato Region|          Nil|       1|     Sealed|      0.0|      0.0|               0.0|        0.0|      50.0|        0.0|       None|0.0| 0.0|      Waipa District|            Nil|          0.0|        0.0|  0.0| 0.0|  0.0|               0.0|Urban|         0.0|    0.0|       0.0|       0|  4596395.0|\n+---+------------+------------+-------+------+---+---------------+---------+-------------------------+-------------------+--------------------+-----------------+-------------+------------------+---------+------+------------------------+-----+----------+-----+--------+---------+---------------+----+-----+----------------+-----+----------+-------------+---------------------+-----------+----------------+--------+-------------+-----------+----------+--------------------+-------------+--------+-----------+---------+---------+------------------+-----------+----------+-----------+-----------+---+----+--------------------+---------------+-------------+-----------+-----+----+-----+------------------+-----+------------+-------+----------+--------+-----------+\nonly showing top 20 rows\n\n"]}}],"execution_count":0},{"cell_type":"code","source":["df_train = df_train.select(\"_c0\",\"crashFinancialYear\",\"speedLimit\",\"crashSeverity\",\"crashSHDescription\",\"roadLane\",\"claimAmount\", \"region\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa08add0-e422-4f10-88ce-c7f7311715d4"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_test = spark.read.csv(\"/FileStore/tables/test.csv\", header=True, inferSchema=True)\ndf_test.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e66a49b-9524-4cfb-95d5-5ea309e52d2f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[26]: 44839","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[26]: 44839"]}}],"execution_count":0},{"cell_type":"code","source":["df_test = df_test.select(\"_c0\",\"crashFinancialYear\",\"speedLimit\",\"crashSeverity\",\"crashSHDescription\",\"roadLane\",\"claimAmount\", \"region\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b301e9d-4a8e-4994-bcb8-1de1173f2179"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df_train.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58aa0cf0-e9ce-46c8-a6d2-a613b21153a9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[28]: 179808","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[28]: 179808"]}}],"execution_count":0},{"cell_type":"code","source":["bootstrapped_data = df_train.sample(withReplacement = True, fraction = 100.0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0890b1c7-ef88-47cc-95ba-381791abb386"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["bootstrapped_data.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19a4461a-0c8c-4a1f-961a-761105a73306"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[30]: 179821453","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[30]: 179821453"]}}],"execution_count":0},{"cell_type":"code","source":["df_train.describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"89de66b5-57d2-47f6-8d06-457c53c65598"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["bootstrapped_data.describe().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cee42a76-de64-4654-9f6b-02774b17af71"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = df_train.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da3a76f3-bc59-4aa4-9da3-425bc9d51d3f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_absolute_error"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0e82b243-221a-4212-8c71-4b3d9e845c72"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df1 = df_test.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35e9f5f3-d5a7-47b3-a06d-16b14f31638f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["model = sm.MixedLM.from_formula(\"claimAmount ~ crashFinancialYear+speedLimit+crashSeverity+crashSHDescription+roadLane\", groups=df[\"region\"], data=df)\nresult = model.fit()\n\n# The BLUPs\nre = result.random_effects\n\n# Multiply each BLUP by the random effects design matrix for one group\nrex = [np.dot(model.exog_re_li[j], re[k]) for (j, k) in enumerate(model.group_labels)]\n\n# Add the fixed and random terms to get the overall prediction\nrex = np.concatenate(rex)\nyp = result.fittedvalues + rex\n\npred = result.predict(exog=df1)\nprint(\"RMSE: \", np.sqrt(np.mean(np.square(pred - df1['claimAmount']))))\nprint(\"MAE: \", mean_absolute_error(pred, df1['claimAmount']))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"747cc4a4-aa47-4887-87fc-91ad63c2d78c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"RMSE:  3585454.3055692245\nMAE:  2065862.8859656514\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["RMSE:  3585454.3055692245\nMAE:  2065862.8859656514\n"]}}],"execution_count":0},{"cell_type":"code","source":["result.summary()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b916a5d2-49a9-4bf8-9010-318ab96f31dc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<table class=\"simpletable\">\n<tr>\n       <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>     <td>claimAmount</td>    \n</tr>\n<tr>\n  <td>No. Observations:</td> <td>179808</td>        <td>Method:</td>              <td>REML</td>        \n</tr>\n<tr>\n     <td>No. Groups:</td>      <td>17</td>          <td>Scale:</td>        <td>12581791591559.6621</td>\n</tr>\n<tr>\n  <td>Min. group size:</td>    <td>657</td>     <td>Log-Likelihood:</td>      <td>-2966894.5945</td>   \n</tr>\n<tr>\n  <td>Max. group size:</td>   <td>52280</td>      <td>Converged:</td>              <td>Yes</td>        \n</tr>\n<tr>\n  <td>Mean group size:</td>  <td>10576.9</td>          <td></td>                    <td></td>          \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n           <td></td>               <th>Coef.</th>      <th>Std.Err.</th>     <th>z</th>    <th>P>|z|</th>   <th>[0.025</th>      <th>0.975]</th>   \n</tr>\n<tr>\n  <th>Intercept</th>            <td>679902.915</td>    <td>55870.438</td> <td>12.169</td>  <td>0.000</td> <td>570398.870</td>  <td>789406.961</td> \n</tr>\n<tr>\n  <th>crashFinancialYear</th>   <td>-29910.961</td>    <td>1368.129</td>  <td>-21.863</td> <td>0.000</td> <td>-32592.446</td>  <td>-27229.477</td> \n</tr>\n<tr>\n  <th>speedLimit</th>            <td>14561.306</td>     <td>423.598</td>  <td>34.375</td>  <td>0.000</td>  <td>13731.070</td>   <td>15391.543</td> \n</tr>\n<tr>\n  <th>crashSeverity</th>        <td>2546871.619</td>   <td>10249.961</td> <td>248.476</td> <td>0.000</td> <td>2526782.064</td> <td>2566961.173</td>\n</tr>\n<tr>\n  <th>crashSHDescription</th>   <td>167777.662</td>    <td>10704.222</td> <td>15.674</td>  <td>0.000</td> <td>146797.773</td>  <td>188757.550</td> \n</tr>\n<tr>\n  <th>roadLane</th>             <td>235524.565</td>    <td>24887.331</td>  <td>9.464</td>  <td>0.000</td> <td>186746.292</td>  <td>284302.838</td> \n</tr>\n<tr>\n  <th>Group Var</th>          <td>18994584282.457</td> <td>2183.417</td>     <td></td>       <td></td>         <td></td>            <td></td>      \n</tr>\n</table>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<table class=\"simpletable\">\n<tr>\n       <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>     <td>claimAmount</td>    \n</tr>\n<tr>\n  <td>No. Observations:</td> <td>179808</td>        <td>Method:</td>              <td>REML</td>        \n</tr>\n<tr>\n     <td>No. Groups:</td>      <td>17</td>          <td>Scale:</td>        <td>12581791591559.6621</td>\n</tr>\n<tr>\n  <td>Min. group size:</td>    <td>657</td>     <td>Log-Likelihood:</td>      <td>-2966894.5945</td>   \n</tr>\n<tr>\n  <td>Max. group size:</td>   <td>52280</td>      <td>Converged:</td>              <td>Yes</td>        \n</tr>\n<tr>\n  <td>Mean group size:</td>  <td>10576.9</td>          <td></td>                    <td></td>          \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n           <td></td>               <th>Coef.</th>      <th>Std.Err.</th>     <th>z</th>    <th>P>|z|</th>   <th>[0.025</th>      <th>0.975]</th>   \n</tr>\n<tr>\n  <th>Intercept</th>            <td>679902.915</td>    <td>55870.438</td> <td>12.169</td>  <td>0.000</td> <td>570398.870</td>  <td>789406.961</td> \n</tr>\n<tr>\n  <th>crashFinancialYear</th>   <td>-29910.961</td>    <td>1368.129</td>  <td>-21.863</td> <td>0.000</td> <td>-32592.446</td>  <td>-27229.477</td> \n</tr>\n<tr>\n  <th>speedLimit</th>            <td>14561.306</td>     <td>423.598</td>  <td>34.375</td>  <td>0.000</td>  <td>13731.070</td>   <td>15391.543</td> \n</tr>\n<tr>\n  <th>crashSeverity</th>        <td>2546871.619</td>   <td>10249.961</td> <td>248.476</td> <td>0.000</td> <td>2526782.064</td> <td>2566961.173</td>\n</tr>\n<tr>\n  <th>crashSHDescription</th>   <td>167777.662</td>    <td>10704.222</td> <td>15.674</td>  <td>0.000</td> <td>146797.773</td>  <td>188757.550</td> \n</tr>\n<tr>\n  <th>roadLane</th>             <td>235524.565</td>    <td>24887.331</td>  <td>9.464</td>  <td>0.000</td> <td>186746.292</td>  <td>284302.838</td> \n</tr>\n<tr>\n  <th>Group Var</th>          <td>18994584282.457</td> <td>2183.417</td>     <td></td>       <td></td>         <td></td>            <td></td>      \n</tr>\n</table>"]}}],"execution_count":0},{"cell_type":"code","source":["bootstrapped_data.createOrReplaceTempView(\"data\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1319bb47-0698-4c5a-ad2c-80ac6400680a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["bootstrapped_data.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f14e7bf6-a81a-4c5f-8ba4-435aaf0efaa9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[39]: 179821453","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[39]: 179821453"]}}],"execution_count":0},{"cell_type":"code","source":["spark_df = spark.sql(\"\"\"\nselect *, _c0%10 as partition_id \nfrom (\n  select *, row_number() over (order by rand()) as user_id\n  from data\n) \n\"\"\")\n\n# preview the results\n#spark_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a48e51ab-b30f-4716-ad3e-e558e734f787"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark_df.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"67177960-1ae2-41a1-95e9-7f557a6e6d99"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[41]: 179821453","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[41]: 179821453"]}}],"execution_count":0},{"cell_type":"code","source":["import time"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2f37746e-6792-4fa7-81d3-046b3638a675"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom pyspark.sql.types import StructType, StructField, LongType, DoubleType\n\n# define a schema for the result set, the user ID and model prediction\nschema = StructType([StructField('user_id', LongType(), True),\n                     StructField('prediction', DoubleType(), True)])  \n\n# define the Pandas UDF \n@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\ndef apply_model(sample_df):\n\n    # run the model on the partitioned data set \n    ids = sample_df['user_id']\n    x_train = sample_df.drop(['user_id', 'partition_id', 'claimAmount'], axis=1)\n    pred = result.predict(exog=sample_df)\n\n    return pd.DataFrame({'user_id': ids, 'prediction': pred[:]})\n    #return pd.DataFrame({'prediction': pred[:]})\n  \n# partition the data and run the UDF  \nresults = spark_df.groupby('partition_id').apply(apply_model)\n\nstart = time.time()\nresults.show()\nprint('time: ',start-time.time())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cc6ca480-ea9d-4c84-bda6-d81d6ea01124"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/spark/python/pyspark/sql/pandas/group_ops.py:81: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n+---------+--------------------+\n|  user_id|          prediction|\n+---------+--------------------+\n|        1|   5220352.261952994|\n|154956903|   4981064.571745215|\n|        5|    4867068.43527849|\n|154956914|  3977265.8482996793|\n|        9|   4156731.615955514|\n|154956916|   4156731.615955514|\n|       12|1.0074807809205348E7|\n|154956936|    4366108.34488732|\n|       13|   4208670.054752502|\n|154956945|    4687245.43516806|\n|       18|   5100708.416849105|\n|154956946|   2882857.372154735|\n|       42|   5399818.029608828|\n|154956956|   9041098.124483839|\n|       56|   4372643.094337186|\n|154956977|   4396019.306163292|\n|       64|  1759414.8036053095|\n|154956996|   9041098.124483839|\n|       90|   9071009.085759811|\n|154957008|  3917443.9257477345|\n+---------+--------------------+\nonly showing top 20 rows\n\ntime:  -820.8498141765594\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/spark/python/pyspark/sql/pandas/group_ops.py:81: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n  warnings.warn(\n+---------+--------------------+\n|  user_id|          prediction|\n+---------+--------------------+\n|        1|   5220352.261952994|\n|154956903|   4981064.571745215|\n|        5|    4867068.43527849|\n|154956914|  3977265.8482996793|\n|        9|   4156731.615955514|\n|154956916|   4156731.615955514|\n|       12|1.0074807809205348E7|\n|154956936|    4366108.34488732|\n|       13|   4208670.054752502|\n|154956945|    4687245.43516806|\n|       18|   5100708.416849105|\n|154956946|   2882857.372154735|\n|       42|   5399818.029608828|\n|154956956|   9041098.124483839|\n|       56|   4372643.094337186|\n|154956977|   4396019.306163292|\n|       64|  1759414.8036053095|\n|154956996|   9041098.124483839|\n|       90|   9071009.085759811|\n|154957008|  3917443.9257477345|\n+---------+--------------------+\nonly showing top 20 rows\n\ntime:  -820.8498141765594\n"]}}],"execution_count":0},{"cell_type":"code","source":["spark_df.count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ea477b3-46bc-43a9-b744-184602ab227a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[44]: 179821453","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[44]: 179821453"]}}],"execution_count":0},{"cell_type":"code","source":["sdf = spark_df.toPandas()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"868d7017-76a2-4c14-9e42-049f56066948"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"/databricks/spark/python/pyspark/sql/pandas/conversion.py:161: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.\n  An error occurred while calling o1849.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 206) (ip-10-172-173-24.us-west-2.compute.internal executor driver): org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\torg.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:91)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:112)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:135)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:237)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:235)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$Lambda$6688/92752362.apply$mcV$sp(Unknown Source)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:238)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:202)\n\tscala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tscala.collection.Iterator.foreach(Iterator.scala:943)\n\tscala.collection.Iterator.foreach$(Iterator.scala:943)\n\tscala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tscala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tscala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:233)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:169)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:162)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:166)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:825)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:828)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\t\tat org.apache.arrow.memory.BaseAllocator.close(BaseAllocator.java:431)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2(ArrowConverters.scala:199)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2$adapted(ArrowConverters.scala:197)\n\t\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:162)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:224)\n\t\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2582)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2565)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2677)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3866)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3870)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3836)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3930)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$6(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:342)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:292)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3928)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3836)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3835)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\torg.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:91)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:112)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:135)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:237)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:235)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$Lambda$6688/92752362.apply$mcV$sp(Unknown Source)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:238)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:202)\n\tscala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tscala.collection.Iterator.foreach(Iterator.scala:943)\n\tscala.collection.Iterator.foreach$(Iterator.scala:943)\n\tscala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tscala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tscala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:233)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:169)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:162)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:166)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:825)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:828)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\t\tat org.apache.arrow.memory.BaseAllocator.close(BaseAllocator.java:431)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2(ArrowConverters.scala:199)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2$adapted(ArrowConverters.scala:197)\n\t\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:162)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:224)\n\t\t... 15 more\n\n  warnings.warn(msg)\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["/databricks/spark/python/pyspark/sql/pandas/conversion.py:161: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true, but has reached the error below and can not continue. Note that 'spark.sql.execution.arrow.pyspark.fallback.enabled' does not have an effect on failures in the middle of computation.\n  An error occurred while calling o1849.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 206) (ip-10-172-173-24.us-west-2.compute.internal executor driver): org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\torg.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:91)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:112)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:135)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:237)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:235)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$Lambda$6688/92752362.apply$mcV$sp(Unknown Source)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:238)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:202)\n\tscala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tscala.collection.Iterator.foreach(Iterator.scala:943)\n\tscala.collection.Iterator.foreach$(Iterator.scala:943)\n\tscala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tscala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tscala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:233)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:169)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:162)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:166)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:825)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:828)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\t\tat org.apache.arrow.memory.BaseAllocator.close(BaseAllocator.java:431)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2(ArrowConverters.scala:199)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2$adapted(ArrowConverters.scala:197)\n\t\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:162)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:224)\n\t\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2582)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2565)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2677)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3866)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3870)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3836)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3930)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$6(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:342)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:292)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3928)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3836)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3835)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\torg.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:91)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:112)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:135)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:237)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:235)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$Lambda$6688/92752362.apply$mcV$sp(Unknown Source)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:238)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:202)\n\tscala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tscala.collection.Iterator.foreach(Iterator.scala:943)\n\tscala.collection.Iterator.foreach$(Iterator.scala:943)\n\tscala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tscala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tscala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:233)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:169)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:162)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:166)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:825)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:828)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\t\tat org.apache.arrow.memory.BaseAllocator.close(BaseAllocator.java:431)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2(ArrowConverters.scala:199)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2$adapted(ArrowConverters.scala:197)\n\t\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:162)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:224)\n\t\t... 15 more\n\n  warnings.warn(msg)\n"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-1791933884705404>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/databricks/utils/instrumentation.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     40\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m             \u001B[0mstart_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m             \u001B[0mreturn_val\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m             \u001B[0mduration\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m1000\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py\u001B[0m in \u001B[0;36mtoPandas\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    115\u001B[0m                     \u001B[0mtmp_column_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'col_{}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m                     \u001B[0mself_destruct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marrowPySparkSelfDestructEnabled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m                     batches = self.toDF(*tmp_column_names)._collect_as_arrow(\n\u001B[0m\u001B[1;32m    118\u001B[0m                         split_batches=self_destruct)\n\u001B[1;32m    119\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatches\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py\u001B[0m in \u001B[0;36m_collect_as_arrow\u001B[0;34m(self, split_batches)\u001B[0m\n\u001B[1;32m    294\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    295\u001B[0m             \u001B[0;31m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 296\u001B[0;31m             \u001B[0mjsocket_auth_server\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetResult\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    297\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    298\u001B[0m         \u001B[0;31m# Separate RecordBatches from batch order indices in results\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1849.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 206) (ip-10-172-173-24.us-west-2.compute.internal executor driver): org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\torg.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:91)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:112)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:135)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:237)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:235)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$Lambda$6688/92752362.apply$mcV$sp(Unknown Source)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:238)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:202)\n\tscala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tscala.collection.Iterator.foreach(Iterator.scala:943)\n\tscala.collection.Iterator.foreach$(Iterator.scala:943)\n\tscala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tscala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tscala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:233)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:169)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:162)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:166)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:825)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:828)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\t\tat org.apache.arrow.memory.BaseAllocator.close(BaseAllocator.java:431)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2(ArrowConverters.scala:199)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2$adapted(ArrowConverters.scala:197)\n\t\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:162)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:224)\n\t\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2582)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2565)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2677)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3866)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3870)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3836)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3930)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$6(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:342)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:292)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3928)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3836)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3835)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\torg.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:91)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:112)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:135)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:237)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:235)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$Lambda$6688/92752362.apply$mcV$sp(Unknown Source)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:238)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:202)\n\tscala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tscala.collection.Iterator.foreach(Iterator.scala:943)\n\tscala.collection.Iterator.foreach$(Iterator.scala:943)\n\tscala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tscala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tscala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:233)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:169)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:162)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:166)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:825)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:828)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\t\tat org.apache.arrow.memory.BaseAllocator.close(BaseAllocator.java:431)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2(ArrowConverters.scala:199)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2$adapted(ArrowConverters.scala:197)\n\t\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:162)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:224)\n\t\t... 15 more\n","errorSummary":"org.apache.spark.SparkException: Exception thrown in awaitResult: ","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-1791933884705404>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0msdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtoPandas\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/spark/python/pyspark/databricks/utils/instrumentation.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     40\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m             \u001B[0mstart_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m             \u001B[0mreturn_val\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m             \u001B[0mduration\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart_time\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0;36m1000\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py\u001B[0m in \u001B[0;36mtoPandas\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    115\u001B[0m                     \u001B[0mtmp_column_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'col_{}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m                     \u001B[0mself_destruct\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_conf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marrowPySparkSelfDestructEnabled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m                     batches = self.toDF(*tmp_column_names)._collect_as_arrow(\n\u001B[0m\u001B[1;32m    118\u001B[0m                         split_batches=self_destruct)\n\u001B[1;32m    119\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatches\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/pandas/conversion.py\u001B[0m in \u001B[0;36m_collect_as_arrow\u001B[0;34m(self, split_batches)\u001B[0m\n\u001B[1;32m    294\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    295\u001B[0m             \u001B[0;31m# Join serving thread and raise any exceptions from collectAsArrowToPython\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 296\u001B[0;31m             \u001B[0mjsocket_auth_server\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetResult\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    297\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    298\u001B[0m         \u001B[0;31m# Separate RecordBatches from batch order indices in results\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    116\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 117\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    118\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    119\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o1849.getResult.\n: org.apache.spark.SparkException: Exception thrown in awaitResult: \n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:428)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:107)\n\tat org.apache.spark.security.SocketAuthServer.getResult(SocketAuthServer.scala:103)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 68.0 failed 1 times, most recent failure: Lost task 0.0 in stage 68.0 (TID 206) (ip-10-172-173-24.us-west-2.compute.internal executor driver): org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\torg.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:91)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:112)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:135)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:237)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:235)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$Lambda$6688/92752362.apply$mcV$sp(Unknown Source)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:238)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:202)\n\tscala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tscala.collection.Iterator.foreach(Iterator.scala:943)\n\tscala.collection.Iterator.foreach$(Iterator.scala:943)\n\tscala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tscala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tscala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:233)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:169)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:162)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:166)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:825)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:828)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\t\tat org.apache.arrow.memory.BaseAllocator.close(BaseAllocator.java:431)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2(ArrowConverters.scala:199)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2$adapted(ArrowConverters.scala:197)\n\t\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:162)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:224)\n\t\t... 15 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2931)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2925)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2925)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1345)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1345)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3193)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3134)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3122)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1107)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2582)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2565)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2677)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$6(Dataset.scala:3866)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3(Dataset.scala:3870)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$3$adapted(Dataset.scala:3836)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3930)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$6(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:342)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:153)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:956)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:115)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:292)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3928)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2(Dataset.scala:3836)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectAsArrowToPython$2$adapted(Dataset.scala:3835)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$2(SocketAuthServer.scala:153)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1(SocketAuthServer.scala:155)\n\tat org.apache.spark.security.SocketAuthServer$.$anonfun$serveToStream$1$adapted(SocketAuthServer.scala:150)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:124)\n\tat org.apache.spark.security.SocketFuncServer.handleConnection(SocketAuthServer.scala:117)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.$anonfun$run$4(SocketAuthServer.scala:70)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:70)\nCaused by: org.apache.spark.util.TaskCompletionListenerException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\nPrevious exception in task: Java heap space\n\tjava.util.Arrays.copyOf(Arrays.java:3236)\n\tjava.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tjava.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tjava.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\torg.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:91)\n\tjava.nio.channels.Channels$WritableByteChannelImpl.write(Channels.java:458)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:112)\n\torg.apache.arrow.vector.ipc.WriteChannel.write(WriteChannel.java:135)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.writeBatchBuffers(MessageSerializer.java:303)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:276)\n\torg.apache.arrow.vector.ipc.message.MessageSerializer.serialize(MessageSerializer.java:237)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.$anonfun$next$1(ArrowConverters.scala:235)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1$$Lambda$6688/92752362.apply$mcV$sp(Unknown Source)\n\tscala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\torg.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:238)\n\torg.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.next(ArrowConverters.scala:202)\n\tscala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n\tscala.collection.Iterator.foreach(Iterator.scala:943)\n\tscala.collection.Iterator.foreach$(Iterator.scala:943)\n\tscala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tscala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tscala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tscala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tscala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tscala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tscala.collection.AbstractIterator.to(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tscala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tscala.collection.AbstractIterator.toBuffer(Iterator.scala:1431)\n\tscala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:233)\n\tat org.apache.spark.TaskContextImpl.invokeTaskCompletionListeners(TaskContextImpl.scala:169)\n\tat org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:162)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:166)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:125)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:95)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:825)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1670)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:828)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.IllegalStateException: Memory was leaked by query. Memory leaked: (402653184)\nAllocator(toBatchIterator) 0/402653184/436207616/9223372036854775807 (res/actual/peak/limit)\n\n\t\tat org.apache.arrow.memory.BaseAllocator.close(BaseAllocator.java:431)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2(ArrowConverters.scala:199)\n\t\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.$anonfun$toBatchIterator$2$adapted(ArrowConverters.scala:197)\n\t\tat org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:162)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.$anonfun$invokeTaskCompletionListeners$1$adapted(TaskContextImpl.scala:169)\n\t\tat org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:224)\n\t\t... 15 more\n"]}}],"execution_count":0},{"cell_type":"code","source":["sdf.shape"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c3d175d-2488-42c7-b277-c610cf4cc0bf"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[66]: (17974190, 9)","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[66]: (17974190, 9)"]}}],"execution_count":0},{"cell_type":"code","source":["result.predict(sdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38fa605f-2fa9-47ff-8c76-f464fe46ffc5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[67]: 0           4.137119e+06\n1           9.250475e+06\n2           4.425930e+06\n3           5.280174e+06\n4           4.425930e+06\n                ...     \n17974185    4.092968e+06\n17974186    5.194204e+06\n17974187    9.041098e+06\n17974188    4.854886e+06\n17974189    4.705331e+06\nLength: 17974190, dtype: float64","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[67]: 0           4.137119e+06\n1           9.250475e+06\n2           4.425930e+06\n3           5.280174e+06\n4           4.425930e+06\n                ...     \n17974185    4.092968e+06\n17974186    5.194204e+06\n17974187    9.041098e+06\n17974188    4.854886e+06\n17974189    4.705331e+06\nLength: 17974190, dtype: float64"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f13ae83-3803-4f01-be7d-da46b8b39de5"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"cas-pyspark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1791933884705384}},"nbformat":4,"nbformat_minor":0}
